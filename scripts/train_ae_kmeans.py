# scripts/train_ae_kmeans.py
"""
AE + K-Means
ì‚¬ì „í•™ìŠµëœ Autoencoderì˜ latent spaceì— K-Means ì ìš©
"""

import os
import numpy as np
import torch
from datetime import datetime

from configs import load_config, create_directories
from data import get_full_dataset, get_fashion_mnist_loaders
from models import create_autoencoder, train_kmeans_baseline


def extract_latent_features(model, X, device, batch_size=256):
    """
    Autoencoderë¡œ latent features ì¶”ì¶œ
    
    Args:
        model: í•™ìŠµëœ Autoencoder
        X: ì…ë ¥ ë°ì´í„° (N, 784)
        device: ë””ë°”ì´ìŠ¤
        batch_size: ë°°ì¹˜ í¬ê¸°
        
    Returns:
        Z: latent features (N, latent_dim)
    """
    model.eval()
    Z_list = []
    
    with torch.no_grad():
        for i in range(0, len(X), batch_size):
            batch = X[i:i+batch_size]
            if isinstance(batch, np.ndarray):
                batch = torch.FloatTensor(batch)
            batch = batch.to(device)
            
            z = model.encode(batch)
            Z_list.append(z.cpu().numpy())
    
    Z = np.concatenate(Z_list, axis=0)
    return Z


def main():
    """AE + K-Means í•™ìŠµ"""
    
    print("\n" + "="*60)
    print("AE + K-Means")
    print("="*60)
    
    # ============================================================
    # 1. ì„¤ì • ë¡œë“œ
    # ============================================================
    config = load_config('configs/config.yaml')
    create_directories(config)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nâœ… Device: {device}")
    
    # ëœë¤ ì‹œë“œ
    seed = config['training']['seed']
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    # ============================================================
    # 2. ë°ì´í„° ë¡œë“œ
    # ============================================================
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©...")
    X, y_true = get_full_dataset(config['data']['data_dir'])
    
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    print(f"   Shape: {X.shape}")
    
    # ============================================================
    # 3. ì‚¬ì „í•™ìŠµëœ Autoencoder ë¡œë“œ
    # ============================================================
    print(f"\nğŸ—ï¸  Autoencoder ë¡œë“œ...")
    
    autoencoder = create_autoencoder(config)
    checkpoint_path = os.path.join(
        config['paths']['checkpoint_dir'],
        'autoencoder_best.pth'
    )
    
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(
            f"ì‚¬ì „í•™ìŠµëœ Autoencoderë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}\n"
            f"ë¨¼ì € 'python scripts/train_autoencoder.py'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
        )
    
    autoencoder.load_state_dict(torch.load(checkpoint_path))
    autoencoder = autoencoder.to(device)
    
    print(f"âœ… Autoencoder ë¡œë“œ ì™„ë£Œ")
    
    # ============================================================
    # 4. Latent features ì¶”ì¶œ
    # ============================================================
    print(f"\nğŸ”„ Latent features ì¶”ì¶œ ì¤‘...")
    start_time = datetime.now()
    
    Z = extract_latent_features(autoencoder, X, device)
    
    elapsed_extract = (datetime.now() - start_time).total_seconds()
    
    print(f"âœ… ì¶”ì¶œ ì™„ë£Œ!")
    print(f"   Original shape: {X.shape}")
    print(f"   Latent shape: {Z.shape}")
    print(f"   ì†Œìš” ì‹œê°„: {elapsed_extract:.2f}ì´ˆ")
    
    # ============================================================
    # 5. K-Means í•™ìŠµ (latent space)
    # ============================================================
    print(f"\nğŸ”„ K-Means í•™ìŠµ (latent space)...")
    start_time = datetime.now()
    
    y_pred, kmeans = train_kmeans_baseline(
        Z,
        n_clusters=config['model']['n_clusters'],
        random_state=seed
    )
    
    elapsed_kmeans = (datetime.now() - start_time).total_seconds()
    
    print(f"âœ… í•™ìŠµ ì™„ë£Œ!")
    print(f"   ì†Œìš” ì‹œê°„: {elapsed_kmeans:.2f}ì´ˆ")
    print(f"   Total ì†Œìš” ì‹œê°„: {elapsed_extract + elapsed_kmeans:.2f}ì´ˆ")
    
    # ============================================================
    # 6. ê²°ê³¼ ì €ì¥
    # ============================================================
    results_dir = config['paths']['results_dir']
    
    # ì˜ˆì¸¡ ê²°ê³¼
    np.save(
        os.path.join(results_dir, 'ae_kmeans_predictions.npy'),
        y_pred
    )
    
    # Latent features
    np.save(
        os.path.join(results_dir, 'ae_latent_features.npy'),
        Z
    )
    
    # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì 
    np.save(
        os.path.join(results_dir, 'ae_kmeans_centers.npy'),
        kmeans.cluster_centers_
    )
    
    # ë©”íƒ€ ì •ë³´
    import json
    meta_info = {
        'n_clusters': config['model']['n_clusters'],
        'latent_dim': Z.shape[1],
        'inertia': float(kmeans.inertia_),
        'n_samples': len(y_pred),
        'extract_time': elapsed_extract,
        'kmeans_time': elapsed_kmeans,
        'total_time': elapsed_extract + elapsed_kmeans,
        'seed': seed
    }
    
    with open(os.path.join(results_dir, 'ae_kmeans_meta.json'), 'w') as f:
        json.dump(meta_info, f, indent=2)
    
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
    print(f"   {results_dir}/ae_kmeans_predictions.npy")
    print(f"   {results_dir}/ae_latent_features.npy")
    print(f"   {results_dir}/ae_kmeans_centers.npy")
    
    # ============================================================
    # 7. í†µê³„
    # ============================================================
    print(f"\nğŸ“Š í´ëŸ¬ìŠ¤í„° ë¶„í¬:")
    unique, counts = np.unique(y_pred, return_counts=True)
    for cluster_id, count in zip(unique, counts):
        print(f"   Cluster {cluster_id}: {count:5d}ê°œ ({count/len(y_pred)*100:.1f}%)")
    
    print("\n" + "="*60)
    print("âœ… AE + K-Means ì™„ë£Œ!")
    print("="*60 + "\n")


if __name__ == "__main__":
    main()