# scripts/train_autoencoder.py
"""
Autoencoder ì‚¬ì „í•™ìŠµ
ì´ë¯¸ì§€ ë³µì› ëŠ¥ë ¥ í•™ìŠµ
"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from datetime import datetime
from tqdm import tqdm

from configs import load_config, create_directories
from data import get_fashion_mnist_loaders
from models import create_autoencoder


def train_epoch(model, train_loader, optimizer, criterion, device):
    """
    1 ì—í¬í¬ í•™ìŠµ
    
    Args:
        model: Autoencoder ëª¨ë¸
        train_loader: í•™ìŠµ ë°ì´í„° ë¡œë”
        optimizer: ì˜µí‹°ë§ˆì´ì €
        criterion: ì†ì‹¤ í•¨ìˆ˜
        device: ë””ë°”ì´ìŠ¤
        
    Returns:
        avg_loss: í‰ê·  ì†ì‹¤
    """
    model.train()
    total_loss = 0
    
    for batch_idx, (x, _) in enumerate(train_loader):
        x = x.to(device)
        
        # Forward
        x_recon = model(x)
        loss = criterion(x_recon, x)
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    avg_loss = total_loss / len(train_loader)
    return avg_loss


def evaluate(model, test_loader, criterion, device):
    """
    í‰ê°€
    
    Args:
        model: Autoencoder ëª¨ë¸
        test_loader: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”
        criterion: ì†ì‹¤ í•¨ìˆ˜
        device: ë””ë°”ì´ìŠ¤
        
    Returns:
        avg_loss: í‰ê·  ì†ì‹¤
    """
    model.eval()
    total_loss = 0
    
    with torch.no_grad():
        for x, _ in test_loader:
            x = x.to(device)
            x_recon = model(x)
            loss = criterion(x_recon, x)
            total_loss += loss.item()
    
    avg_loss = total_loss / len(test_loader)
    return avg_loss


def main():
    """Autoencoder ì‚¬ì „í•™ìŠµ"""
    
    print("\n" + "="*60)
    print("Autoencoder ì‚¬ì „í•™ìŠµ")
    print("="*60)
    
    # ============================================================
    # 1. ì„¤ì • ë¡œë“œ
    # ============================================================
    config = load_config('configs/config.yaml')
    create_directories(config)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nâœ… Device: {device}")
    
    # ëœë¤ ì‹œë“œ
    seed = config['training']['seed']
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
    
    # ============================================================
    # 2. ë°ì´í„° ë¡œë“œ
    # ============================================================
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©...")
    train_loader, test_loader = get_fashion_mnist_loaders(
        batch_size=config['data']['batch_size'],
        data_dir=config['data']['data_dir']
    )
    
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    print(f"   Train batches: {len(train_loader)}")
    print(f"   Test batches: {len(test_loader)}")
    
    # ============================================================
    # 3. ëª¨ë¸ ìƒì„±
    # ============================================================
    print(f"\nğŸ—ï¸  ëª¨ë¸ ìƒì„±...")
    model = create_autoencoder(config)
    model = model.to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    print(f"   Parameters: {total_params:,}ê°œ")
    
    # ============================================================
    # 4. í•™ìŠµ ì„¤ì •
    # ============================================================
    criterion = nn.MSELoss()
    optimizer = optim.Adam(
        model.parameters(),
        lr=config['training']['pretrain_lr']
    )
    
    num_epochs = config['training']['pretrain_epochs']
    
    print(f"\nâš™ï¸  í•™ìŠµ ì„¤ì •:")
    print(f"   Epochs: {num_epochs}")
    print(f"   Learning rate: {config['training']['pretrain_lr']}")
    print(f"   Batch size: {config['data']['batch_size']}")
    
    # ============================================================
    # 5. í•™ìŠµ ë£¨í”„
    # ============================================================
    print(f"\nğŸ”„ í•™ìŠµ ì‹œì‘...")
    print("-" * 60)
    
    best_loss = float('inf')
    history = {'train_loss': [], 'test_loss': []}
    
    start_time = datetime.now()
    
    for epoch in range(1, num_epochs + 1):
        # í•™ìŠµ
        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
        
        # í‰ê°€
        test_loss = evaluate(model, test_loader, criterion, device)
        
        # ê¸°ë¡
        history['train_loss'].append(train_loss)
        history['test_loss'].append(test_loss)
        
        # ì¶œë ¥
        print(f"Epoch [{epoch:3d}/{num_epochs}] "
              f"Train Loss: {train_loss:.6f} | "
              f"Test Loss: {test_loss:.6f}")
        
        # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥
        if test_loss < best_loss:
            best_loss = test_loss
            checkpoint_path = os.path.join(
                config['paths']['checkpoint_dir'],
                'autoencoder_best.pth'
            )
            torch.save(model.state_dict(), checkpoint_path)
    
    elapsed_time = (datetime.now() - start_time).total_seconds()
    
    print("-" * 60)
    print(f"âœ… í•™ìŠµ ì™„ë£Œ!")
    print(f"   ì†Œìš” ì‹œê°„: {elapsed_time/60:.2f}ë¶„")
    print(f"   Best Test Loss: {best_loss:.6f}")
    
    # ============================================================
    # 6. ìµœì¢… ëª¨ë¸ ì €ì¥
    # ============================================================
    final_path = os.path.join(
        config['paths']['checkpoint_dir'],
        'autoencoder_final.pth'
    )
    torch.save(model.state_dict(), final_path)
    
    # History ì €ì¥
    np.save(
        os.path.join(config['paths']['results_dir'], 'ae_history.npy'),
        history
    )
    
    print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ:")
    print(f"   {checkpoint_path}")
    print(f"   {final_path}")
    
    print("\n" + "="*60)
    print("âœ… Autoencoder ì‚¬ì „í•™ìŠµ ì™„ë£Œ!")
    print("="*60 + "\n")


if __name__ == "__main__":
    main()