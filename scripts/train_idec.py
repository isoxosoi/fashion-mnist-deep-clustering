# scripts/train_idec.py
"""
IDEC í•™ìŠµ
Autoencoder + Clustering ë™ì‹œ ìµœì í™”
"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from datetime import datetime
from sklearn.cluster import KMeans

from configs import load_config, create_directories
from data import get_fashion_mnist_loaders, get_full_dataset
from models import create_autoencoder, create_idec


def initialize_cluster_centers(model, data_loader, n_clusters, device):
    """
    K-Meansë¡œ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ ì´ˆê¸°í™”
    
    Args:
        model: IDEC ëª¨ë¸
        data_loader: ë°ì´í„° ë¡œë”
        n_clusters: í´ëŸ¬ìŠ¤í„° ê°œìˆ˜
        device: ë””ë°”ì´ìŠ¤
        
    Returns:
        cluster_centers: ì´ˆê¸°í™”ëœ ì¤‘ì‹¬ì  (n_clusters, latent_dim)
    """
    print("\nğŸ”„ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ ì´ˆê¸°í™” ì¤‘...")
    
    model.eval()
    latents = []
    
    # ëª¨ë“  ë°ì´í„°ì˜ latent vector ì¶”ì¶œ
    with torch.no_grad():
        for x, _ in data_loader:
            x = x.to(device)
            z = model.encoder(x)
            latents.append(z.cpu().numpy())
    
    latents = np.concatenate(latents, axis=0)
    
    # K-Meansë¡œ ì´ˆê¸°í™”
    kmeans = KMeans(n_clusters=n_clusters, n_init=20)
    kmeans.fit(latents)
    
    print(f"âœ… í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ ì´ˆê¸°í™” ì™„ë£Œ (Inertia: {kmeans.inertia_:.2f})")
    
    return torch.tensor(kmeans.cluster_centers_, dtype=torch.float32)


def train_epoch(model, train_loader, optimizer, device, gamma):
    """
    1 ì—í¬í¬ í•™ìŠµ
    
    Args:
        model: IDEC ëª¨ë¸
        train_loader: í•™ìŠµ ë°ì´í„° ë¡œë”
        optimizer: ì˜µí‹°ë§ˆì´ì €
        device: ë””ë°”ì´ìŠ¤
        gamma: í´ëŸ¬ìŠ¤í„°ë§ ì†ì‹¤ ê°€ì¤‘ì¹˜
        
    Returns:
        avg_total_loss: í‰ê·  ì „ì²´ ì†ì‹¤
        avg_recon_loss: í‰ê·  ë³µì› ì†ì‹¤
        avg_cluster_loss: í‰ê·  í´ëŸ¬ìŠ¤í„°ë§ ì†ì‹¤
    """
    model.train()
    
    total_loss_sum = 0
    recon_loss_sum = 0
    cluster_loss_sum = 0
    
    for x, _ in train_loader:
        x = x.to(device)
        
        # Forward
        x_recon, q, z = model(x)
        
        # Target distribution
        p = model.target_distribution(q).detach()
        
        # ì†ì‹¤ ê³„ì‚°
        # 1. ë³µì› ì†ì‹¤ (MSE)
        recon_loss = nn.MSELoss()(x_recon, x)
        
        # 2. í´ëŸ¬ìŠ¤í„°ë§ ì†ì‹¤ (KL divergence)
        cluster_loss = nn.KLDivLoss(reduction='batchmean')(
            torch.log(q), 
            p
        )
        
        # 3. ì „ì²´ ì†ì‹¤
        total_loss = recon_loss + gamma * cluster_loss
        
        # Backward
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()
        
        # ê¸°ë¡
        total_loss_sum += total_loss.item()
        recon_loss_sum += recon_loss.item()
        cluster_loss_sum += cluster_loss.item()
    
    n_batches = len(train_loader)
    return (
        total_loss_sum / n_batches,
        recon_loss_sum / n_batches,
        cluster_loss_sum / n_batches
    )


def evaluate(model, test_loader, device):
    """
    í‰ê°€
    
    Args:
        model: IDEC ëª¨ë¸
        test_loader: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”
        device: ë””ë°”ì´ìŠ¤
        
    Returns:
        predictions: ì˜ˆì¸¡ ë¼ë²¨
    """
    model.eval()
    predictions = []
    
    with torch.no_grad():
        for x, _ in test_loader:
            x = x.to(device)
            pred = model.predict(x)
            predictions.append(pred.cpu().numpy())
    
    predictions = np.concatenate(predictions, axis=0)
    return predictions


def main():
    """IDEC í•™ìŠµ"""
    
    print("\n" + "="*60)
    print("IDEC í•™ìŠµ")
    print("="*60)
    
    # ============================================================
    # 1. ì„¤ì • ë¡œë“œ
    # ============================================================
    config = load_config('configs/config.yaml')
    create_directories(config)
    
    # ë””ë°”ì´ìŠ¤
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nâœ… Device: {device}")
    
    # ëœë¤ ì‹œë“œ
    seed = config['training']['seed']
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
    
    # ============================================================
    # 2. ë°ì´í„° ë¡œë“œ
    # ============================================================
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©...")
    train_loader, test_loader = get_fashion_mnist_loaders(
        batch_size=config['data']['batch_size'],
        data_dir=config['data']['data_dir']
    )
    
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    
    # ============================================================
    # 3. Autoencoder ë¡œë“œ
    # ============================================================
    print(f"\nğŸ—ï¸  ì‚¬ì „í•™ìŠµëœ Autoencoder ë¡œë“œ...")
    
    autoencoder = create_autoencoder(config)
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint_path = os.path.join(
        config['paths']['checkpoint_dir'],
        'autoencoder_best.pth'
    )
    
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(
            f"ì‚¬ì „í•™ìŠµëœ Autoencoderë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}\n"
            f"ë¨¼ì € 'python scripts/train_autoencoder.py'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
        )
    
    autoencoder.load_state_dict(torch.load(checkpoint_path))
    autoencoder = autoencoder.to(device)
    
    print(f"âœ… Autoencoder ë¡œë“œ ì™„ë£Œ")
    
    # ============================================================
    # 4. IDEC ëª¨ë¸ ìƒì„±
    # ============================================================
    print(f"\nğŸ—ï¸  IDEC ëª¨ë¸ ìƒì„±...")
    model = create_idec(autoencoder, config)
    model = model.to(device)
    
    print(f"âœ… IDEC ìƒì„± ì™„ë£Œ")
    
    # ============================================================
    # 5. í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ ì´ˆê¸°í™”
    # ============================================================
    cluster_centers = initialize_cluster_centers(
        model, train_loader, 
        config['model']['n_clusters'],
        device
    )
    model.cluster_centers.data = cluster_centers.to(device)
    
    # ============================================================
    # 6. í•™ìŠµ ì„¤ì •
    # ============================================================
    optimizer = optim.SGD(
        model.parameters(),
        lr=config['training']['finetune_lr'],
        momentum=0.9
    )
    
    num_epochs = config['training']['finetune_epochs']
    gamma = config['training']['gamma']
    
    print(f"\nâš™ï¸  í•™ìŠµ ì„¤ì •:")
    print(f"   Epochs: {num_epochs}")
    print(f"   Learning rate: {config['training']['finetune_lr']}")
    print(f"   Gamma: {gamma}")
    
    # ============================================================
    # 7. í•™ìŠµ ë£¨í”„
    # ============================================================
    print(f"\nğŸ”„ í•™ìŠµ ì‹œì‘...")
    print("-" * 80)
    
    history = {
        'total_loss': [],
        'recon_loss': [],
        'cluster_loss': []
    }
    
    start_time = datetime.now()
    
    for epoch in range(1, num_epochs + 1):
        # í•™ìŠµ
        total_loss, recon_loss, cluster_loss = train_epoch(
            model, train_loader, optimizer, device, gamma
        )
        
        # ê¸°ë¡
        history['total_loss'].append(total_loss)
        history['recon_loss'].append(recon_loss)
        history['cluster_loss'].append(cluster_loss)
        
        # ì¶œë ¥
        print(f"Epoch [{epoch:3d}/{num_epochs}] "
              f"Total: {total_loss:.6f} | "
              f"Recon: {recon_loss:.6f} | "
              f"Cluster: {cluster_loss:.6f}")
        
        # ì£¼ê¸°ì ìœ¼ë¡œ ì˜ˆì¸¡ ì €ì¥
        if epoch % 10 == 0 or epoch == num_epochs:
            predictions = evaluate(model, test_loader, device)
            np.save(
                os.path.join(
                    config['paths']['results_dir'],
                    f'idec_predictions_epoch{epoch}.npy'
                ),
                predictions
            )
    
    elapsed_time = (datetime.now() - start_time).total_seconds()
    
    print("-" * 80)
    print(f"âœ… í•™ìŠµ ì™„ë£Œ!")
    print(f"   ì†Œìš” ì‹œê°„: {elapsed_time/60:.2f}ë¶„")
    
    # ============================================================
    # 8. ëª¨ë¸ ì €ì¥
    # ============================================================
    model_path = os.path.join(
        config['paths']['checkpoint_dir'],
        'idec_final.pth'
    )
    torch.save(model.state_dict(), model_path)
    
    # History ì €ì¥
    np.save(
        os.path.join(config['paths']['results_dir'], 'idec_history.npy'),
        history
    )
    
    print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ:")
    print(f"   {model_path}")
    
    print("\n" + "="*60)
    print("âœ… IDEC í•™ìŠµ ì™„ë£Œ!")
    print("="*60 + "\n")


if __name__ == "__main__":
    main()