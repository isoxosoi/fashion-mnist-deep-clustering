"""
IDEC (Improved Deep Embedded Clustering)
Autoencoder + K-Meansë¥¼ ë™ì‹œì— í•™ìŠµ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class IDEC(nn.Module):
    """
    IDEC: ë³µì› + êµ°ì§‘í™”ë¥¼ ë™ì‹œì— í•™ìŠµ
    
    êµ¬ì¡°:
        - Encoder + Decoder (Autoencoder)
        - Cluster Centers (êµ°ì§‘ ì¤‘ì‹¬ì ë“¤)
    """
    
    def __init__(self, autoencoder, n_clusters=10, alpha=1.0):
        """
        Args:
            autoencoder: ì‚¬ì „í•™ìŠµëœ Autoencoder
            n_clusters: êµ°ì§‘ ê°œìˆ˜ (ê¸°ë³¸ê°’: 10)
            alpha: Student t-distribution íŒŒë¼ë¯¸í„°
        """
        super(IDEC, self).__init__()
        
        # Autoencoderì—ì„œ Encoderì™€ Decoder ê°€ì ¸ì˜¤ê¸°
        self.encoder = autoencoder.encoder
        self.decoder = autoencoder.decoder
        
        self.n_clusters = n_clusters
        self.alpha = alpha
        
        # êµ°ì§‘ ì¤‘ì‹¬ì ë“¤ (í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°)
        # shape: (n_clusters, latent_dim)
        latent_dim = autoencoder.latent_dim
        self.cluster_centers = nn.Parameter(torch.Tensor(n_clusters, latent_dim))
        
        # ì´ˆê¸°í™”: Xavier Normal
        torch.nn.init.xavier_normal_(self.cluster_centers.data)
    
    def forward(self, x):
        """
        ìˆœì „íŒŒ
        
        Args:
            x: ì…ë ¥ ì´ë¯¸ì§€ (batch_size, 784)
            
        Returns:
            x_recon: ë³µì›ëœ ì´ë¯¸ì§€ (batch_size, 784)
            q: êµ°ì§‘ í• ë‹¹ í™•ë¥  (batch_size, n_clusters)
            z: ì ì¬ ë²¡í„° (batch_size, latent_dim)
        """
        # 1. ì¸ì½”ë”©
        z = self.encoder(x)
        
        # 2. ë””ì½”ë”© (ë³µì›)
        x_recon = self.decoder(z)
        
        # 3. êµ°ì§‘ í• ë‹¹ í™•ë¥  ê³„ì‚°
        q = self.soft_assignment(z)
        
        return x_recon, q, z
    
    def soft_assignment(self, z):
        """
        Student t-distributionìœ¼ë¡œ êµ°ì§‘ í• ë‹¹ í™•ë¥  ê³„ì‚°
        
        Args:
            z: ì ì¬ ë²¡í„° (batch_size, latent_dim)
            
        Returns:
            q: ê° ë°ì´í„°ê°€ ê° êµ°ì§‘ì— ì†í•  í™•ë¥  (batch_size, n_clusters)
        """
        # zì™€ ê° êµ°ì§‘ ì¤‘ì‹¬ ì‚¬ì´ì˜ ê±°ë¦¬ ê³„ì‚°
        # z: (batch_size, latent_dim)
        # cluster_centers: (n_clusters, latent_dim)
        
        # ê±°ë¦¬ ê³„ì‚°: ||z_i - mu_j||^2
        # unsqueezeë¡œ ì°¨ì› ë§ì¶”ê¸°
        # z.unsqueeze(1): (batch_size, 1, latent_dim)
        # cluster_centers: (1, n_clusters, latent_dim)
        
        distances = torch.sum(
            (z.unsqueeze(1) - self.cluster_centers) ** 2, 
            dim=2
        )  # (batch_size, n_clusters)
        
        # Student t-distribution
        # q_ij = (1 + ||z_i - mu_j||^2 / alpha)^(-(alpha+1)/2)
        q = 1.0 / (1.0 + distances / self.alpha)
        q = q.pow((self.alpha + 1.0) / 2.0)
        
        # ì •ê·œí™” (ê° í–‰ì˜ í•©ì´ 1ì´ ë˜ë„ë¡)
        q = q / torch.sum(q, dim=1, keepdim=True)
        
        return q
    
    def target_distribution(self, q):
        """
        Target distribution (P) ê³„ì‚°
        
        Args:
            q: í˜„ì¬ êµ°ì§‘ í• ë‹¹ í™•ë¥  (batch_size, n_clusters)
            
        Returns:
            p: ëª©í‘œ ë¶„í¬ (batch_size, n_clusters)
        """
        # p_ij = q_ij^2 / sum_i(q_ij) / sum_j(q_ij^2 / sum_i(q_ij))
        
        # 1. q^2 ê³„ì‚°
        weight = q ** 2
        
        # 2. ê° ì—´(í´ëŸ¬ìŠ¤í„°)ì˜ í•©ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
        weight = weight / torch.sum(q, dim=0, keepdim=True)
        
        # 3. ì •ê·œí™”
        p = weight / torch.sum(weight, dim=1, keepdim=True)
        
        return p
    
    def predict(self, x):
        """
        êµ°ì§‘ ì˜ˆì¸¡
        
        Args:
            x: ì…ë ¥ ì´ë¯¸ì§€ (batch_size, 784)
            
        Returns:
            predicted_labels: ì˜ˆì¸¡ëœ êµ°ì§‘ ë²ˆí˜¸ (batch_size,)
        """
        _, q, _ = self.forward(x)
        return torch.argmax(q, dim=1)


# ============================================================
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ============================================================
if __name__ == "__main__":
    from autoencoder import Autoencoder
    
    print("="*60)
    print("IDEC ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    # 1. Autoencoder ìƒì„±
    ae = Autoencoder(input_dim=784, latent_dim=10)
    print(f"\nâœ… Autoencoder ìƒì„± ì™„ë£Œ")
    
    # 2. IDEC ëª¨ë¸ ìƒì„±
    model = IDEC(ae, n_clusters=10)
    print(f"âœ… IDEC ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    print(f"   êµ°ì§‘ ê°œìˆ˜: 10")
    print(f"   í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ shape: {model.cluster_centers.shape}")
    
    # 3. ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸
    batch_size = 32
    x = torch.randn(batch_size, 784)
    
    print(f"\nğŸ”„ ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸:")
    print(f"   ì…ë ¥ shape: {x.shape}")
    
    x_recon, q, z = model(x)
    
    print(f"   ë³µì› ì´ë¯¸ì§€ shape: {x_recon.shape}")
    print(f"   êµ°ì§‘ í™•ë¥  shape: {q.shape}")
    print(f"   ì ì¬ ë²¡í„° shape: {z.shape}")
    
    # 4. êµ°ì§‘ í• ë‹¹ í™•ë¥  í™•ì¸
    print(f"\nğŸ“Š ì²« ë²ˆì§¸ ìƒ˜í”Œì˜ êµ°ì§‘ í™•ë¥ :")
    print(f"   {q[0].detach().numpy()}")
    print(f"   í•©ê³„: {torch.sum(q[0]).item():.4f} (1ì— ê°€ê¹Œì›Œì•¼ í•¨)")
    
    # 5. êµ°ì§‘ ì˜ˆì¸¡
    pred_labels = model.predict(x)
    print(f"\nğŸ¯ ì˜ˆì¸¡ëœ êµ°ì§‘:")
    print(f"   {pred_labels[:10].numpy()}")
    
    # 6. Target distribution í…ŒìŠ¤íŠ¸
    p = model.target_distribution(q)
    print(f"\nğŸ“ˆ Target distribution shape: {p.shape}")
    
    print("\n" + "="*60)
    print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
    print("="*60)